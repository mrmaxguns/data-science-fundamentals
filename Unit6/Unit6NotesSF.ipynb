{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsDPkrwYcWRarhq6/y7+1Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedarredondo/data-science-fundamentals/blob/main/Unit6/Unit6NotesSF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gzOuXakudTC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import arviz as az\n",
        "import pymc as pm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 6: Multiple Generalized Linear Modeling\n",
        "\n",
        "Last unit we learned how to predict a probalistic process using a different probablistic process--like using temperature to predict bike rentals.\n",
        "\n",
        "This unit, we'll see what happens when we allow ourselves to use multiple predictors, instead of just one. This is more powerful, but also more challenging; we'll now have to decide what and how many predictors to use. We'll learn some new techniques and concepts to help us choose predictors and whole models, and we'll even use new libraries, bambi and kulprit, to make the model creation and checking process faster and more streamlined.\n",
        "\n",
        "In the midst of all that, we'll introduce and dicuss the Under/Overfitting issue, which is the constant balancing act in modeling.\n",
        "\n",
        "We'll also play with categorical variables, heirarchies, and introduce interactions. These are all ways to discover the relationship between different predictors.\n",
        "\n",
        "Here's what we'll cover, and the order we'll cover it in:\n",
        "1. Multiple Linear Regression with pymc and model comparison\n",
        "2. Regression with Bambi\n",
        "3. Balancing Underfitting with Overfitting\n",
        "  - Variable selection (possibly with kulprit)\n",
        "5. Distributional models (varable variance\n",
        "6. Categorical variables versus Hierarchies\n",
        "7. Interactions\n",
        "\n",
        "Let's dive in."
      ],
      "metadata": {
        "id": "eb1w9VZBuxgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll spend most of this unit with the bikes data set, so that we can focus on the new concepts. I'll then throw in examples with other data sets once we've introduced all the new things"
      ],
      "metadata": {
        "id": "qnZs4DCRxB7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bikes = pd.read_csv('https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/bikes.csv')"
      ],
      "metadata": {
        "id": "K-esvwemxW9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bikes"
      ],
      "metadata": {
        "id": "T5hzuXqjxh0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Linear Modeling with PyMC + Model Comparison with LOO\n",
        "\n",
        "We will only use base PyMC for multiple linear modeling briefly. This is because multiple linear regression requires more plotting than simple linear regression, and bambi has some nice features for quickly and easily generating those plots.\n",
        "\n",
        "We will use this oppurtunity to inroduce a new way to compare the performance of models: elpd_loo. This new method gives us a nice one number summary of which model is better, which is a useful tool to have when our models get two multidimensional to easily parse."
      ],
      "metadata": {
        "id": "pM6ogLmVyOZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've already seen the bikes model with temperature predicting rented. The model below is almost exactly the same."
      ],
      "metadata": {
        "id": "8fhn5Xy20zb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nbb stands for negative binomial bike model\n",
        "with pm.Model() as model_nbb:\n",
        "\n",
        "    #priors for the linear part of the model\n",
        "    α = pm.Normal(\"α\", mu=0, sigma=1)\n",
        "    β = pm.Normal(\"β\", mu=0, sigma=10)\n",
        "\n",
        "    #the linear part of our model,\n",
        "    #but with a twist:\n",
        "    #our line is exponentiated, in order to make our all our values positive\n",
        "    μ = pm.Deterministic(\"μ\", pm.math.exp(α + β * bikes.temperature))\n",
        "\n",
        "    #prior for the likelihood's standard deviation\n",
        "    σ = pm.HalfNormal(\"σ\", 10)\n",
        "\n",
        "    #likelihood\n",
        "    y_pred = pm.NegativeBinomial(\"y\", mu=μ, alpha=σ, observed=bikes.rented)\n",
        "\n",
        "    #we need the log likelihood for model comparison later\n",
        "    idata_nbb = pm.sample(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "fnt-n5kn2pOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm.sample_posterior_predictive(idata_nbb, model = model_nbb, extend_inferencedata=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ISLHWPf_4Ah7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model below is almost exactly the same."
      ],
      "metadata": {
        "id": "HWU6Fn6n2qwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mlb stands for multiple linear bikes\n",
        "with pm.Model() as model_mlb:\n",
        "\n",
        "    #priors on the linear part of the model\n",
        "    α = pm.Normal(\"α\", mu=0, sigma=1)\n",
        "    β0 = pm.Normal(\"β0\", mu=0, sigma=10)\n",
        "    β1 = pm.Normal(\"β1\", mu=0, sigma=10)\n",
        "\n",
        "    #linear part of the model\n",
        "    μ = pm.Deterministic(\"μ\", pm.math.exp(α + β0 * bikes.temperature + β1 * bikes.hour))\n",
        "\n",
        "    #prior for the standard deviation\n",
        "    σ = pm.HalfNormal(\"σ\", 10)\n",
        "\n",
        "    #likelihood\n",
        "    y = pm.NegativeBinomial(\"y\", mu=μ, alpha=σ, observed=bikes.rented)\n",
        "\n",
        "    #we need the log likelihood for model comparison later\n",
        "    idata_mlb = pm.sample(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "UiqXvxg51NBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm.sample_posterior_predictive(idata_mlb, model = model_mlb, extend_inferencedata=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "atFs1wjk3JbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task1**:\n",
        "\n",
        "Looking only at the code, what's the difference between model\\_nbb and model\\_mlb?"
      ],
      "metadata": {
        "id": "5HePkNrS4FzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer1**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "ZiVWdFrz4Y39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the two models' preformance using a ppc"
      ],
      "metadata": {
        "id": "SVJqJwqj46By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_ppc(idata_nbb, num_pp_samples=200, colors=[\"C1\", \"C0\", \"C1\"])"
      ],
      "metadata": {
        "id": "quLNaOY_4wYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_ppc(idata_mlb, num_pp_samples=200, colors=[\"C1\", \"C0\", \"C1\"])"
      ],
      "metadata": {
        "id": "BXJR-tMQ3Dff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task2**:\n",
        "\n",
        "Describe any differences you can see."
      ],
      "metadata": {
        "id": "Isr4ha2_AVyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer2**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "EabNLToHAjeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully you're wondering if there is another way to compare these two models.\n",
        "\n",
        "And there is!\n",
        "\n",
        "The method is called Pareto Smoothed Importance Sampling Leave-One-Out Cross-Validation, which estimates the Expected Log-Pointwise-predictive Density.\n",
        "\n",
        "We abbreviate all that with ELPD$_{LOO-CV}$, or elpd_loo, or even just LOO.\n",
        "\n",
        "As the long full name implies, there are a lot of advanced, fancy tricks applied to the posterior to calculate LOO.\n",
        "\n",
        "The important things to know:\n",
        "- a more positive elpd_loo is better, but only in comparison to another model.\n",
        "- the standard error (SE), sorta like the standard deviation, of elpd_loo helps tell us whether two models are significantly different than one another in terms of performance.\n",
        "- There's something called a Pareto k diagnostic that tells us whether there were too many influential points for elpd_loo to be effective.\n",
        "\n"
      ],
      "metadata": {
        "id": "-XDAMI6qBo9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the k diagnostic for the multiple linear model."
      ],
      "metadata": {
        "id": "qphICmvyRLvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.loo(idata_mlb)"
      ],
      "metadata": {
        "id": "tU0-ox3D83e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All 348 values in the bikes data set have a small enough influence that we can almost completely trust elpd_loo's recommendation.\n",
        "\n",
        "We also get the value of elpd_loo, and its standard error. But these are useless without another model to compare them too."
      ],
      "metadata": {
        "id": "AdwBOlq6RU29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do have another model to compare though! And arviz has some nice functions that will make the comparison trivial."
      ],
      "metadata": {
        "id": "n-oK0_7lSZOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cmp_df stands for compare dataframe\n",
        "cmp_df = az.compare( {\"multi_lin\":idata_mlb,\"single_lin\":idata_nbb} )\n",
        "\n",
        "#0 is the best rank; we want a lower elpd_loo\n",
        "cmp_df"
      ],
      "metadata": {
        "id": "nPgXmGrn75ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "elpd_loo and se are easier to understand using the graph below.\n",
        "\n",
        "The other relevant entries are warning, which just needs to say False--False means our k diagnostic was good enough. The other interesting entry is weight. This can be used to average the models being compared, using the following code:\n",
        "\n",
        "```\n",
        "idata_w = az.weight_predictions([idata_mlb,idata_nbb], weights=[0.903435, 0.096565])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Qid5N0xdTF1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we don't want to combine the models though, but choose between them. Then its often easier to graph the above table, then make a decision."
      ],
      "metadata": {
        "id": "wK8rxGl7VUpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_compare(cmp_df)"
      ],
      "metadata": {
        "id": "a0DVdFiQ-cLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task3**:\n",
        "\n",
        "Using the model comparion table, which model would you pick? Why?"
      ],
      "metadata": {
        "id": "krOoMG61VilG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer3**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "Vm8bL_NpVs3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bambi is Best\n",
        "For (generalized) linear models, anyway.\n",
        "\n",
        "Bambi is library for building bayesian (generalized) linear models.\n",
        "\n",
        "Here are the two main two reasons to love bambi:\n",
        "\n",
        "- by auto assigning normal and half normal priors with mean 0 and sd 1, bambi models are fast to write and read.\n",
        "- it has built in methods for plotting the posterior means and posterior predictive distribution. No more giant blocks of code to create plots for linear models!\n",
        "\n",
        "Here are the two main reasons to be cautious with bambi:\n",
        "\n",
        "- it only works with (generalized) linear models. As in, using bambi means that we are assuming some type of linear model.\n",
        "- bambi auto assigns normal and half normal priors with a mean of 0 and sd of 1. This isn't that big a deal, since there's a quick way to feed the priors we want into bambi. Its easy to get lazy with bambi, which is fine, until it isn't."
      ],
      "metadata": {
        "id": "uhVIfB99X2Xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab also doesn't have bambi preinstalled, so we need to install it each time we pull up colab.\n",
        "\n",
        "We will also need to install the latest version of xarray, since bambi relies on its latest features.\n",
        "\n",
        "Note that you may get prompted to restart the session, especially if you've already run pymc and/or arviz. Go ahead and do so, if prompted."
      ],
      "metadata": {
        "id": "eyB7BF6ynz9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/pydata/xarray.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HKVx351yml5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr"
      ],
      "metadata": {
        "id": "Ar2GzF1WPNoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bambi"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FB8Wdy8GYTL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bambi as bmb"
      ],
      "metadata": {
        "id": "PG0tF4j_YP8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "E2JUdnuwo-xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Bikes + Regularizing Priors\n",
        "\n",
        "Let's start by remaking the two bike models from earlier in the unit, but with bambi.\n",
        "\n",
        "We'll also discuss the concept of regularizing priors; specifically, we'll see the the weakly informative priors bambi defaults too oftenn work great in practice."
      ],
      "metadata": {
        "id": "cOpC2SIKd0sY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bambi bikes (1 predictor)\n",
        "We'll start with the single variable model."
      ],
      "metadata": {
        "id": "Nd-NUqqdeBkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we put the priors that aren't normal or half normal with mean 0 and sigma 1\n",
        "#in a dict\n",
        "priors ={\"temperature\" : bmb.Prior(\"Normal\", mu=0,sigma=10), \"alpha\" : bmb.Prior(\"HalfNormal\",sigma=10)}\n",
        "\n",
        "#This creates a skeleton of the model; it hasn't run MCMC or created a posterior yet\n",
        "model_nbb_bmb = bmb.Model(\"rented ~ temperature\", bikes, family = \"negativebinomial\", priors=priors)"
      ],
      "metadata": {
        "id": "qaIBqB6zfQBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_nbb_bmb"
      ],
      "metadata": {
        "id": "JTVKZewGvKrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'family' argument in bmb.Model() is what determines both our likelihood and our link function. Bambi has default link functions that work well with families of likelihoods. Note the mu = log(x) is the same as exp(mu)=x.\n",
        "\n",
        "Specifying 'family' is optional; the default is a normal likelihood.\n",
        "\n",
        "Also notice that defining the model only takes two lines of code, and we only need one more to make the model it self."
      ],
      "metadata": {
        "id": "fTRpufTyqlZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is bambi's equivalent of pm.sample()\n",
        "idata_nbb_bmb = model_nbb_bmb.fit(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "N_-5Q8_8gJ2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two more lines gives us a plot of the posterior mean."
      ],
      "metadata": {
        "id": "Y8iN6U_BrLXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_nbb_bmb, idata_nbb_bmb, \"temperature\")\n",
        "plt.plot(bikes.temperature, bikes.rented, \"C2.\", zorder=-3)"
      ],
      "metadata": {
        "id": "HX0oQLKnlsPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A final two lines give us the posterior predictive distribution.\n",
        "\n",
        "In both graphs, the shaded blue area is a 94% HDI. You can change this by using the prob argument in the plot_predictions method."
      ],
      "metadata": {
        "id": "ng_VNLwYrQwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_nbb_bmb, idata_nbb_bmb, \"temperature\", pps=True)\n",
        "plt.plot(bikes.temperature, bikes.rented, \"C2.\", zorder=-3)"
      ],
      "metadata": {
        "id": "yTb1AppTmRyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regularizing Priors (weakly informative priors)\n",
        "\n",
        "We do not have to specify priors get a bambi model to run; bambi has a routine to determine the priors based off the raw data. Remember, that's the same strategy I told you to employ when you needed to specify prior(s) for which you had no context.\n",
        "\n",
        "So, if you're handed data you know next to nothing about, you can safely selected a likelihood, and fire the model up.\n",
        "\n",
        "But if you do have an understanding of the context, your final model should reflect your knowledge in its priors."
      ],
      "metadata": {
        "id": "LvgOggV2tlWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#notice that the priors argument isn't specified\n",
        "model_nbb_bmb_regpriors = bmb.Model(\"rented ~ temperature\", bikes, family = \"negativebinomial\")\n",
        "idata_nbb_bmb_regpriors = model_nbb_bmb_regpriors.fit(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "arKOMn3iw_GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you do let bambi select priors for you, then you must print out the model, to see what it selected. That way, you can check if its selections jive with your inuition."
      ],
      "metadata": {
        "id": "0LdNfY2vxRc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_nbb_bmb_regpriors"
      ],
      "metadata": {
        "id": "j2Vu3qjSxNff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the posterior mean and posterior predictive."
      ],
      "metadata": {
        "id": "-G17eOHhxig9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_nbb_bmb_regpriors, idata_nbb_bmb_regpriors, \"temperature\")\n",
        "plt.plot(bikes.temperature, bikes.rented, \"C2.\", zorder=-3)"
      ],
      "metadata": {
        "id": "wmZuW2s5xiI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_nbb_bmb_regpriors, idata_nbb_bmb_regpriors, \"temperature\", pps = True)\n",
        "plt.plot(bikes.temperature, bikes.rented, \"C2.\", zorder=-3)"
      ],
      "metadata": {
        "id": "7cdc0FKjxxuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task4**:\n",
        "\n",
        "Are their any significant differences in the graphs of model_nbb_bmb and model_nbb_regpriors?\n",
        "\n",
        "Why or why not?"
      ],
      "metadata": {
        "id": "y2DRy8hXyZwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer4**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "SMVvHOHQywnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bambi bikes (multiple predictors)\n",
        "\n",
        "Now that we know what a regularings prior is, and why they're often good enough let's recreate the model with both temperature and hour.\n",
        "\n",
        "And since I was using the data to create my priors anyway, I let bambi choose my priors."
      ],
      "metadata": {
        "id": "BA1UAaIBsHuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_mlb_bmb = bmb.Model(\"rented ~ temperature + hour\", bikes, family=\"negativebinomial\")\n",
        "idata_mlb_bmb = model_mlb_bmb.fit(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "F7LJfwmZuJA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I need to print the model, so I can see what priors bambi went with."
      ],
      "metadata": {
        "id": "kvQLUeQe0FeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_mlb_bmb"
      ],
      "metadata": {
        "id": "5UTVt-oAuZO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the model, so we can actually see what it suggests."
      ],
      "metadata": {
        "id": "zxaUXGyJ0QVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_mlb_bmb, idata_mlb_bmb, [\"temperature\", \"hour\"],\n",
        "                               subplot_kwargs={\"group\":None, \"panel\":\"hour\"},\n",
        "                               legend=False,\n",
        "                               fig_kwargs={\"sharey\":True, \"sharex\":True})"
      ],
      "metadata": {
        "id": "9jQKJSTE0X3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task5**:\n",
        "\n",
        "Describe the relationship between the slope of temperature and the hour of the day, in model_mlb_bmb."
      ],
      "metadata": {
        "id": "7_ObTF7A5dWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer5**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "I0GNxR1L5ohz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bambi models are equally compatible with arivz, since they are running pymc under the hood. These means everything we learned about elpd_loo still applies."
      ],
      "metadata": {
        "id": "ohf3EI35-2j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmp_df_bmb = az.compare( {\"multi_lin\":idata_mlb_bmb,\"single_lin\":idata_nbb_bmb} )\n",
        "cmp_df_bmb"
      ],
      "metadata": {
        "id": "P8aSM7_m-BS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_compare(cmp_df_bmb)"
      ],
      "metadata": {
        "id": "tUZCAb-X-BPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Underfiting versus Overfitting\n",
        "\n",
        "When using multiple predictors, some logical questions arise:\n",
        "- do we really need multiple predictors?\n",
        "- if so, how many do we need?\n",
        "- and which ones? Are some better than others? The same as others?\n",
        "\n",
        "The process of answering those questions is known as variable selection."
      ],
      "metadata": {
        "id": "H6trQtjl_One"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task6**:\n",
        "\n",
        "Given what you know and what we've learned thus far, how would you go about selecting which variables are best? If it helps, imagine we used all the variables in the bikes data set to predict rented."
      ],
      "metadata": {
        "id": "CG148LXNA8n2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer6**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "0Eaf5ePeBWNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kulprit is a library that makes variable selection--using the methods and skills we've learned--easier and more streamlined.\n",
        "\n",
        "As has become usual, we need to restart the session, download kulprit, and then reimport everything."
      ],
      "metadata": {
        "id": "Hl70cfpYB-vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/bambinos/kulprit.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Dvxta2jk_YH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/pydata/xarray.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YZ2xKPHz_8Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bambi"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1-A7d2uT__r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kulprit as kpt\n",
        "import xarray as xr\n",
        "import bambi as bmb"
      ],
      "metadata": {
        "id": "ZKtcElFv_Tkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "GpdNUFvL_1rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that are libraries are ready to run, let's import the data we'll use"
      ],
      "metadata": {
        "id": "C-zK-wKMCuRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "body = pd.read_csv('https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/body_fat.csv')"
      ],
      "metadata": {
        "id": "AXY9Nve4Czsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "body"
      ],
      "metadata": {
        "id": "O-7L0Ui2C44n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The body data set has a measure of body fat percentage (siri), as well as several other measurments of other parts of a person's body, plus their age.\n",
        "\n",
        "To see which variables predict siri best, we need to makea  model that includes all the variables.\n",
        "\n",
        "We will take as a given that a normal likelihood, and bambi's choice of weakly informative priors, are good enough. Even if those assumptions later turn out to be bad, there's nothing wrong with starting with the defaults of bambi, and seeing what they get you"
      ],
      "metadata": {
        "id": "lMFHdWUYC7OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_body = bmb.Model(\"siri ~ age + weight + height + abdomen + thigh + wrist\", data=body)\n",
        "idata_body = model_body.fit(idata_kwargs={'log_likelihood': True})"
      ],
      "metadata": {
        "id": "cDDUgeGoC64o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#always print and read the model description when you use the defaults\n",
        "model_body"
      ],
      "metadata": {
        "id": "OUI_tvhSDoAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kulprit is going to perform something call Projection Predictive Inference (ppi). Basically, kulprit will use elpd_loo to find the subset of variables that:\n",
        "- contains the least number of variables\n",
        "- also has a relatively low elpd_loo\n",
        "\n",
        "We already know that elpd_loo is the standard for model comparison.\n",
        "\n",
        "The reason we want the least number of variables has to do with overfitting. Any model can be made arbitraily better fit a given data set by adding more predictors.\n",
        "\n",
        "The trick we need to pull off in modeling is to make a model that uses the given data to predict new data. For that reason, we usually have a bias agaisnt adding more variables. There's some advanced methods related to Gausissian processes (the next two units) that can avoid this problem though.\n",
        "\n",
        "The second reason to avoid more variables: some of the variables might be giving the same information, or canceling each other out in some way we don't understand. This is always a potential problem, and means we will always want to perform variable selection when we have more than one predictor."
      ],
      "metadata": {
        "id": "VdYtE7gxEnUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this let's kulprit know which model and what posterior to use.\n",
        "ppi = kpt.ProjectionPredictive(model_body,idata_body)"
      ],
      "metadata": {
        "id": "h-_eVkeqEgdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run all the relevant submodels from the variables that we have.\n",
        "ppi.search()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pcnhj6O9EgUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this tells me the name and comosition of the submodels,\n",
        "#whose elpd_loo is vizualized in the next graph.\n",
        "ppi"
      ],
      "metadata": {
        "id": "mUUM_VbQXi-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comparison plot for the elpd_loo of all the submodels from above\n",
        "cmp, ax = ppi.plot_compare(plot=True, figsize=(11, 4))"
      ],
      "metadata": {
        "id": "ip_EBcgCX7Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task7**:\n",
        "\n",
        "Using the graphs above, determine which combination of variables is the best choice to make a model to predict the siri variable. Explicitly mention how you took underfitting and overfitting into account."
      ],
      "metadata": {
        "id": "aA3KuewAYuc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer7**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "n4q5kgyNZNqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task8**:\n",
        "\n",
        "Will kulprit always put the variables in the same order of priority? Check with at least three other people, or run the model and kulprit three more times."
      ],
      "metadata": {
        "id": "bgWB3XcGeVcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer8**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "gutMvmWbfDi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distributional models: Dealing with variable variance\n",
        "\n",
        "We'll now take a quite detour back to single predictor land, so that I can show you how to account for variable variance with bambi.\n",
        "\n",
        "When we build a linear regression model, there is at least one linear equation applied in our formulas--usually to the location/center parameter of our likelihood, which we've been calling mu.\n",
        "\n",
        "Applying a linear equation to more than one parameter of our likelihood is called a distributional model. We've done this before with the babies dataset. We'll do it again now, but use bambi.\n"
      ],
      "metadata": {
        "id": "mFNTnE_eRXo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "babies = pd.read_csv(\"https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/babies.csv\")"
      ],
      "metadata": {
        "id": "joU3GpFTTY_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we need to specify the two regression relationships with a bambi method\n",
        "formula_babies = bmb.Formula(\n",
        "    \"length ~ np.sqrt(month)\",\n",
        "    \"sigma ~ month\"\n",
        ")\n",
        "\n",
        "#dis for distributional model\n",
        "model_dis = bmb.Model(formula_babies, babies)\n",
        "\n",
        "#I only need \"idata_kwargs={\"log_likelihood\":True}\"\" if I'm going to use\n",
        "#elpd_loo for variable selection. So I could have dropped it here\n",
        "idata_dis = model_dis.fit(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "jopUcVr8Tna7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shows me what priors/likelihood bambi used.\n",
        "model_dis"
      ],
      "metadata": {
        "id": "M4hxFeF2ik5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#there's more code here so I could plot two HDIs\n",
        "_, ax = plt.subplots(sharey=True, sharex=\"col\", figsize=(12, 6))\n",
        "#mean line\n",
        "bmb.interpret.plot_predictions(model_dis, idata_dis, \"month\", ax=ax, fig_kwargs={\"color\":\"k\"})\n",
        "#94% HDI\n",
        "bmb.interpret.plot_predictions(model_dis, idata_dis, \"month\", pps=True, ax=ax)\n",
        "#65% HDI\n",
        "ax_ = bmb.interpret.plot_predictions(model_dis, idata_dis, \"month\", pps=True, ax=ax, prob=0.65)\n",
        "ax_[1][0].get_children()[5].set_facecolor('C1')\n",
        "\n",
        "#raw data\n",
        "ax.plot(babies.month, babies.length, \"C2.\", zorder=-3)"
      ],
      "metadata": {
        "id": "5fmZzN-pUn5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same thing as in the previous unit. Neat."
      ],
      "metadata": {
        "id": "Zt-0y-ixj1aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical variables, Hierarchies, and Interactions\n",
        "\n",
        "With the basics of multiple linear regression and bambi behind us, we can now look at the various ways variables can intereact. A tad onfusingly, only one of these methods is called an interaction.\n",
        "\n",
        "We'll recreate:\n",
        "- see how categorical variables work with multiple linear regression\n",
        "- examine hierearchies through bambi\n",
        "- introduce interactions, in light of categorical variables and hierarchies.\n",
        "\n",
        "Note: all priors will be regularizing priors, or weakly informative priors, that bambi will select based on the data itself.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hIWXKrtcFfJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical variables versus Hierarchies\n",
        "\n",
        "Let's get back to the action something new: creating a model with categorical variables and quantitative variables. Sepcifically, one categorical and several quantitative variables.\n",
        "\n",
        "We'll also create a hierarchical regression with that same categorical variable, and discuss when to treat it as its own variable, and when to think of it as a hierarchy."
      ],
      "metadata": {
        "id": "NofZjz_vj4TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#have to drop incomplete rows, so that bambi will run\n",
        "basketball = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/basketball2324.csv').dropna()"
      ],
      "metadata": {
        "id": "kfVxvltJlOa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#only look at players who played more than 400 minutes\n",
        "basketball = basketball.query('MP > 400')\n",
        "#remove players who never missed a free throw\n",
        "basketball = basketball.query('`FT%` != 1.0')"
      ],
      "metadata": {
        "id": "oevLDINKridn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter out the combo positions. This while make it easier to read the graphs\n",
        "basketball = basketball.query(\"Pos in ['C','PF','SF','SG','PG']\")"
      ],
      "metadata": {
        "id": "2Rb6cpo3oa0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model\n",
        "model_basketball = bmb.Model(\"`FG%` ~ `FT%` + Pos\", data=basketball)\n",
        "#fit the model\n",
        "idata_basketball = model_basketball.fit(idata_kwargs={'log_likelihood': True})"
      ],
      "metadata": {
        "id": "mW9I54himlhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot 94% HDIs of the means of each position\n",
        "bmb.interpret.plot_predictions(model_basketball,\n",
        "                               idata_basketball,\n",
        "                                [\"FT%\",  \"Pos\"], fig_kwargs={\"figsize\":(11, 4)})"
      ],
      "metadata": {
        "id": "0rpA5H7xn5Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before explaining what's going on, I'm goin to jump right in the using Pos as a hierarchy."
      ],
      "metadata": {
        "id": "lIIfE5hHpmoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model, but as a hierarchy\n",
        "model_basketball_h = bmb.Model(\"`FG%` ~ (`FT%`|Pos)\", data=basketball)\n",
        "#create the model\n",
        "idata_basketball_h = model_basketball_h.fit(idata_kwargs={'log_likelihood': True})"
      ],
      "metadata": {
        "id": "XBqYP_1jp1KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_basketball_h,\n",
        "                               idata_basketball_h,\n",
        "                               [\"FT%\",\"Pos\"],\n",
        "                               fig_kwargs={\"figsize\":(11, 4)})"
      ],
      "metadata": {
        "id": "fCVnH0hZqId3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task9**:\n",
        "\n",
        "Identify and explain any similarities or differences in the plots of model_basketball and model_basketball_h."
      ],
      "metadata": {
        "id": "gPvrC5Tepmkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer9**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "RZK-3mJDtkQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interactions\n",
        "\n",
        "It's often the case that the effect of a predictor on the respone variable is affected by a different predictor variable.\n",
        "\n",
        "I'll continue to use the basketball data set for this, and I'll use one example with a categorical/quantitative variable combo, and one with two quantitative variables."
      ],
      "metadata": {
        "id": "9TKIn2_2w2eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model; bb_inter stands for baskeball interaction\n",
        "model_bb_inter = bmb.Model(\"`FG%` ~ `FT%` + Pos + `FT%`:Pos\", data=basketball)\n",
        "#create the model\n",
        "idata_bb_inter = model_bb_inter.fit(idata_kwargs={'log_likelihood': True})"
      ],
      "metadata": {
        "id": "6gi6LKlAx4lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_bb_inter,\n",
        "                               idata_bb_inter,\n",
        "                               [\"FT%\",\"Pos\"],\n",
        "                               fig_kwargs={\"figsize\":(11, 4)})"
      ],
      "metadata": {
        "id": "5GpdBjiYySNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task10**:\n",
        "\n",
        "Compare this with the previous two graphs. How has the interaction affected the slope of FT% with respect to FG%?"
      ],
      "metadata": {
        "id": "ECtRec20zfbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer10**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "zjYjIuWDzw-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do an interaction effect with all quantiative variables. For this model, I'll just replace Pos with 3PA."
      ],
      "metadata": {
        "id": "gYCieHPQ1l0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model; bb_interq stands for baskeball interaction quantiative\n",
        "model_bb_interq = bmb.Model(\"`FG%` ~ `FT%` + `3PA` + `FT%`:`3PA`\", data=basketball)\n",
        "#create the model\n",
        "idata_bb_interq = model_bb_interq.fit(idata_kwargs={'log_likelihood': True})"
      ],
      "metadata": {
        "id": "zlJ2pt3c1Tv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_bb_interq,\n",
        "                               idata_bb_interq,\n",
        "                               [\"FT%\",\"3PA\"],\n",
        "                               fig_kwargs={\"figsize\":(11, 13)})#,\n",
        "                               #legend=False)"
      ],
      "metadata": {
        "id": "gMz3-AM411eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task11**:\n",
        "\n",
        "Using the plot above, describe the effect of 3PA on slope of FT% with respect to FG%."
      ],
      "metadata": {
        "id": "SlobQtlq10Ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer11**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "t5VGMcng3qYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task12**:\n",
        "\n",
        "Using the visualizations of model_bb_inter and model_bb_interq, answer the following:\n",
        "1. Do centers (C) have, on average, low 3PA compared to other positions?\n",
        "2. Is FT% a good predictor of FG%?"
      ],
      "metadata": {
        "id": "K-TofbXx4wqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer12**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "U53R4N-656GI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "As usual, we've covered a lot of ground in one unit.\n",
        "\n",
        "We:\n",
        "\n",
        "- learned how to use multiple predictors at once, both in vanilla pymc and using a new library, bambi.\n",
        "- saw that having multiple predictors made model selection much more challenging, and we practiced using a new metric--elpd_loo--to help us decide between models\n",
        "- further complicated model selection with variable selection, and the notion of Under/Overfitting. We learned that:\n",
        "  - our priors have been helping with overfitting all along,\n",
        "  - elpd_loo helps pick a model that doesn't underfit too much.\n",
        "  - we should have a bias towards simpler models, to further guard agaisnt overfitting, and to help make our models more interpretable.\n",
        "- Practiced comparing and contrasting the predictions and fit of categorical varibles, hierarchies, and interactions.\n",
        "\n",
        "And now I say, congradulations! You have almost all the basics of generalized linear models at your disposal now. This is a big deal; GLMs are often a good approximation of many processes, and they are often interpretable. There's plenty more to learn, but the paths in front of you have now become much more varied.\n",
        "\n",
        "In the next unit, we'll cover the reamining basics of glms (polynomial regression, b splines) in order to give you inuition for the most powerful modeling tool of all, a tool from which nerual networks are but one instance of: Gaussian Processes."
      ],
      "metadata": {
        "id": "0KvIVeo0S8wL"
      }
    }
  ]
}