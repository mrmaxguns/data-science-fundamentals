{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQhh3sR2S4h73xOsTiixmh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedarredondo/data-science-fundamentals/blob/main/Unit5/Unit5NotesSF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HOcY--6MSLS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import arviz as az\n",
        "import xarray as xr\n",
        "import pymc as pm\n",
        "\n",
        "from scipy.interpolate import PchipInterpolator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unit 5: Modeling with Lines (1 predictor)\n",
        "\n",
        " Up until now, we've used data from something to predict itself; we've used Wembanyama's shooting free throws to predict him shooting free throws, soccer playes shooting goals to predict soccer plays shooting goals, amino acid chemcial shift vales to predict amino acid chemcial shift values.\n",
        "\n",
        "In this unit, we will use data from one process to predict data from different process. The first example we'll see is using the temperature on a given day to predict how many bicycles are rented on that same day. We'll use that example to introduce:\n",
        "- how to relate temperature and bike rentals with a straight line trend.\n",
        "- how to improve our predictions by filtering our straight line through different likelihoods.\n",
        "\n",
        "We'll then switch to predicting the length of babies using their age in months. This will present new challenges:\n",
        "- how do we account for changing variation in baby length?\n",
        "- do we always have to use straight lines for prediction, or can our lines be curvy?\n",
        "\n",
        "I'll then return to the NBA context in order to show how to apply a hierarchical model structure in a linear model.\n",
        "\n",
        "Finally, I'll do an example of how to use a linear model to classify things; in this case, flowers based on the lengths of their sepals (leaves).\n",
        "\n",
        "Got all that? No? Well, no worries--we'll step through it all, one thing at a time."
      ],
      "metadata": {
        "id": "nvEqvtEbMoN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Straight Lines, Bikes, and Likelihoods\n",
        "\n",
        "How does tempeture affect bike rentals in a city? I'm not sure, but I do have access to our textbooks data set that has the temperature and number of bikes rented for a give day. Let's check it out."
      ],
      "metadata": {
        "id": "k6zeF3RkplYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data into a pandas dataframe, and print it\n",
        "url_bikes = 'https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/bikes.csv'\n",
        "#I could have put the url string directly into pd.read_csv()\n",
        "bikes = pd.read_csv(url_bikes)"
      ],
      "metadata": {
        "id": "draeQo0trWwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bikes"
      ],
      "metadata": {
        "id": "Db4qd4c-SKK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there's more than just temperature and rented bike counts in there; we'll use the other variables in a later unit.\n",
        "\n",
        "For now, let's graph the two variables we're interested in: temperature on the x axis, and rented count on the y axis."
      ],
      "metadata": {
        "id": "-OZZeAuZqzv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scatter plot of temperature v. rented count of bikes\n",
        "bikes.plot(x=\"temperature\", y=\"rented\", figsize=(12, 3), kind=\"scatter\")"
      ],
      "metadata": {
        "id": "dLg6UwJJr4dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditionally, we statisticians call x our predictor and y the predicted. I think its more useful to use temperature to predict the number of rented bikes, but the only reason we're not going the other direction is because I think that's silly in context--there's no math reason we're using temperature to predict number of bikes."
      ],
      "metadata": {
        "id": "lgaq1Ybxr-ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anyway, before we get to our model, let's talk about what a line is, and why they're so darn useful.\n",
        "\n",
        "For us, a line will be anything satifying this relationship: $y=mx + b$. Hopefully that's familiar from the algebra classes you've taken.\n",
        "We're not actually gonna use those letters though; instead we'll write our lines as: $\\hat{y}=α+βx$.\n",
        "\n",
        "I'll call $\\hat{y}$ y_hat, or y_pred. $α$ is known as a (y) intercept, and I'll refer to $β$ as a slope or a coefficient."
      ],
      "metadata": {
        "id": "bIFxJEFFs882"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task1**:\n",
        "\n",
        "What do the letters in $\\hat{y}=α+βx$ tell me about my line?\n",
        "\n",
        "How is that helpful for using temperature to predict the number of rented bikes on a given day? Why use a line?"
      ],
      "metadata": {
        "id": "tLkFtxpHuNIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know why lines are useful, and what assumptions we make when we use them--let's use them!"
      ],
      "metadata": {
        "id": "AJKgNcYky65p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lb stands for linear bike\n",
        "with pm.Model() as model_lb:\n",
        "\n",
        "    #priors for the linear part of our model\n",
        "    α = pm.Normal(\"α\", mu=0, sigma=100)\n",
        "    β = pm.Normal(\"β\", mu=0, sigma=10)\n",
        "\n",
        "    #this is the linear part of our model\n",
        "    μ = pm.Deterministic(\"μ\", α + β * bikes.temperature)\n",
        "\n",
        "    #prior for the standard deviation of our likelihood\n",
        "    #Cauchy is a T dist with nu = 1\n",
        "    σ = pm.HalfCauchy(\"σ\", 10)\n",
        "\n",
        "    #likelihood\n",
        "    y_pred = pm.Normal(\"y_pred\", mu=μ, sigma=σ, observed=bikes.rented)\n",
        "\n",
        "    #inference data object\n",
        "    idata_lb = pm.sample()"
      ],
      "metadata": {
        "id": "hhAC0Wc4zJ8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2**:\n",
        "\n",
        "Compare and contrast this linear model structure from the models we've used in the past units. I recommend making a krusche diagram to help yourself out."
      ],
      "metadata": {
        "id": "kJhH8vNLzQFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've looked at how the model specification is new and different, let's see what line our model dreamed up."
      ],
      "metadata": {
        "id": "TzF9IFTE41sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot of the posteriors for all the components of my line\n",
        "#'~μ' means plot all variables except μ\n",
        "az.plot_posterior(idata_lb, var_names=[\"~μ\"], figsize=(12, 3))"
      ],
      "metadata": {
        "id": "-VGU4Gd05BfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grabs 100 posterior samples\n",
        "posterior = az.extract(idata_lb, num_samples=100)\n",
        "\n",
        "# grabbing x values for graphing.\n",
        "x_plot = xr.DataArray(\n",
        "    np.linspace(bikes.temperature.min(), bikes.temperature.max(), 50),\n",
        "    dims=\"plot_id\"\n",
        "    )\n",
        "\n",
        "# this creates the expected line, the path we predict temperature and\n",
        "#rented bike count deviates from\n",
        "mean_line = posterior[\"α\"].mean() + posterior[\"β\"].mean() * x_plot\n",
        "\n",
        "#These are a 50 lines that our model came up with\n",
        "lines = posterior[\"α\"] + posterior[\"β\"] * x_plot\n",
        "\n",
        "#set up plot\n",
        "_, ax = plt.subplots()\n",
        "\n",
        "#plots 50 other lines our model came up with.\n",
        "ax.plot(x_plot, lines.T, c=\"C1\", alpha=0.2, label=\"lines\")\n",
        "\n",
        "#plots the mean line\n",
        "ax.plot(x_plot, mean_line, c=\"C0\", label=\"mean line\")\n",
        "\n",
        "#plot the raw data\n",
        "ax.plot(bikes.temperature, bikes.rented, \"C2.\", zorder=-3)\n",
        "\n",
        "#label axes and create legend\n",
        "ax.set_xlabel(\"temperature\")\n",
        "ax.set_ylabel(\"rented bikes\")"
      ],
      "metadata": {
        "id": "Ws7PHtTE5VSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**:\n",
        "The above lines, including the orange ones, only represent the mean of our posterior. These lines represeent what our model expects our data to look like, if it really is modeled well by a straight line."
      ],
      "metadata": {
        "id": "rMqXzSXawEOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task3**:\n",
        "\n",
        "Using only the posterior visualizations above as a reference, does the linear model with normal likelihood do a good job explaining the relationship between temperature and number of rented bikes?"
      ],
      "metadata": {
        "id": "tvKCp51n5Tad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've seen the mean, let's see the distribution of our models predictions.\n",
        "\n",
        "Focus on the graph themselves for now. You can return to parse through the code that generates the graph later."
      ],
      "metadata": {
        "id": "lY8jrdVosJ3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# posterior predictive check\n",
        "pm.sample_posterior_predictive(idata_lb, model=model_lb,  extend_inferencedata=True)"
      ],
      "metadata": {
        "id": "kQ0qJVpA8yXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plots the posterior predictive distribution\n",
        "\n",
        "#grabs the expected line, or line of best fit formula\n",
        "mean_line = idata_lb.posterior[\"μ\"].mean((\"chain\", \"draw\"))\n",
        "\n",
        "#creates some x values to run through our line formula\n",
        "temperatures = np.random.normal(bikes.temperature.values, 0.01)\n",
        "idx = np.argsort(temperatures)\n",
        "x = np.linspace(temperatures.min(), temperatures.max(), 15)\n",
        "\n",
        "#grabs the 94% HDI and 50% HDI, and sets them up for graphing.\n",
        "y_pred_q = idata_lb.posterior_predictive[\"y_pred\"].quantile(\n",
        "    [0.03, 0.97, 0.25, 0.75], dim=[\"chain\", \"draw\"]\n",
        ")\n",
        "y_hat_bounds = iter(\n",
        "    [\n",
        "        PchipInterpolator(temperatures[idx], y_pred_q[i][idx])(x)\n",
        "        for i in range(4)\n",
        "    ]\n",
        ")\n",
        "\n",
        "#plots raw data and our line of best fit\n",
        "_, ax = plt.subplots()\n",
        "ax.plot(bikes.temperature, bikes.rented, \"C2.\", zorder=-3)\n",
        "ax.plot(bikes.temperature[idx], mean_line[idx], c=\"C0\")\n",
        "\n",
        "\n",
        "#graphs the 94% and 50% HDIs\n",
        "for lb, ub in zip(y_hat_bounds, y_hat_bounds):\n",
        "    ax.fill_between(x, lb, ub, color=\"C1\", alpha=0.5)\n",
        "\n",
        "#labels\n",
        "ax.set_xlabel(\"temperature\")\n",
        "ax.set_ylabel(\"rented bikes\")"
      ],
      "metadata": {
        "id": "5hCaCnf67HsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how our model performs by assessing its predictions."
      ],
      "metadata": {
        "id": "G1iHksPw6u9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#posterior predictive check\n",
        "az.plot_ppc(idata_lb, num_pp_samples=200, colors=[\"C1\", \"C0\", \"C1\"])"
      ],
      "metadata": {
        "id": "vaRQ523VQFWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task4**:\n",
        "\n",
        "How does the normal likelihood linear model's posterior predictive distribution perform?"
      ],
      "metadata": {
        "id": "7QlkcQz57RFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task5**:\n",
        "\n",
        "Find a [likelihood](https://www.pymc.io/projects/docs/en/stable/api/distributions.html) that works better for the bikes data. Show that it works better by running the pymc model and doing a posterior predictive check.\n",
        "\n",
        "*Hint*: Try plotting ```bikes.rented``` in order to see the shape of the raw data more clearly. Also: is the number of bikes discrete or continuous?"
      ],
      "metadata": {
        "id": "j-5rXWQ58Cpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalizing the Linear Model\n",
        "\n",
        "That last model with the negative binomial likelihood is an example of a Generlized Linear Model (glm).\n",
        "\n",
        "Linear models can have any likelihood, as long as the data is processed thorugh a linear function: $μ=α+βx$. We often denote the likelihood as $\\phi(f(μ),θ)$, where $θ$ is all the priors we need, $μ$ is the linear function $α+βx$. In the normal model, $f(μ)=1̇⋅μ=μ$.\n",
        "\n",
        "That changed when we used the negative binomial likelihood. A negative binomial's support is all positive; a common and quick way to transform a linear function into a function that's all positive is to apply it as an exponential power.\n",
        "\n",
        "Like this: $e^{α+βx}$. That's what we did in the code\n",
        "``` pm.math.exp(α + β * bikes.temperature) ``` .\n",
        "Remember, temperature from the bikes data set is our predictor, x.\n",
        "\n",
        "\n",
        "Anytime we shove $α+βx$ into a function before sliding it into our likelihood, we are generalizing our linear model. Usually we do this to make our chosen likelihood work better, as we did with the negative binomial example.\n",
        "\n",
        "But we can generalize our line even with a normal likelihood. Let's see an example.\n"
      ],
      "metadata": {
        "id": "oE1-s2zDK96B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data into a pandas dataframe, and print it\n",
        "url_babies = 'https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/babies.csv'\n",
        "#I could have put the url string directly into pd.read_csv()\n",
        "babies = pd.read_csv(url_babies)"
      ],
      "metadata": {
        "id": "CVc4lrD9Nk5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "babies.plot.scatter('month','length')"
      ],
      "metadata": {
        "id": "PwZxLgNdXqfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is the lengths (heights) and ages of newborn/toddler baby girls, collected from the World Health Organization, and cleaned by the author of our textbook."
      ],
      "metadata": {
        "id": "jtVfM8cmY-DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task6**:\n",
        "\n",
        "Write a linear model with normal likelihood that uses a babies age in months to predict their length. Do not transform the linear part (yet).\n",
        "\n",
        "Then use a posterior predictive check to assess how well our model fits our data, and comment on the fit."
      ],
      "metadata": {
        "id": "BzELEKW7gGcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task7**:\n",
        "\n",
        "Write a linear model with normal likelihood that uses a babies age in months to predict their length--but transfrom the linear part of our model to better fit the data. Make sure to use a function within the [PyMC math library](https://www.pymc.io/projects/docs/en/stable/api/math.html)\n",
        "\n",
        "Then, use a posterior predictive check to assess how good our model fit is."
      ],
      "metadata": {
        "id": "nyDtexqadOCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how to alter our model to account for the apparent trend that baby lengths get more variable as babies age."
      ],
      "metadata": {
        "id": "QseC3oDobKlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vvba for variable variance babies\n",
        "with pm.Model() as model_vvba:\n",
        "\n",
        "    #priors for line for our mean\n",
        "    α = pm.Normal(\"α\", sigma=10)\n",
        "    β = pm.Normal(\"β\", sigma=10)\n",
        "\n",
        "    #priors for the line for our standard deviation\n",
        "    γ = pm.HalfNormal(\"γ\", sigma=10)\n",
        "    δ = pm.HalfNormal(\"δ\", sigma=10)\n",
        "\n",
        "    #mean line\n",
        "    μ = pm.Deterministic(\"μ\", α + β * pm.math.sqrt(babies.month))\n",
        "\n",
        "    #standard deviation line; this allows our variance to vary over th x values\n",
        "    σ = pm.Deterministic(\"σ\", γ + δ * babies.month)\n",
        "\n",
        "    #normal likelihood\n",
        "    y_pred = pm.Normal(\"y_pred\", mu=μ, sigma=σ, observed=babies.length)\n",
        "\n",
        "    idata_vvba = pm.sample()"
      ],
      "metadata": {
        "id": "b5FXwFccZnjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plots the posterior\n",
        "\n",
        "_, ax = plt.subplots()\n",
        "\n",
        "ax.plot(babies.month, babies.length, \"C0.\", alpha=0.1)\n",
        "\n",
        "posterior_vvba = az.extract(idata_vvba)\n",
        "\n",
        "μ_m = posterior_vvba[\"μ\"].mean(\"sample\").values\n",
        "σ_m = posterior_vvba[\"σ\"].mean(\"sample\").values\n",
        "\n",
        "ax.plot(babies.month, μ_m, c=\"k\")\n",
        "ax.fill_between(babies.month, μ_m + 1 * σ_m, μ_m - 1 * σ_m, alpha=0.6, color=\"C1\")\n",
        "ax.fill_between(babies.month, μ_m + 2 * σ_m, μ_m - 2 * σ_m, alpha=0.4, color=\"C1\")\n",
        "\n",
        "ax.set_xlabel(\"months\")\n",
        "ax.set_ylabel(\"length\")"
      ],
      "metadata": {
        "id": "QGnhgZq7t7Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creates our posterior predictive distribution\n",
        "pm.sample_posterior_predictive(\n",
        "    idata_vvba, model = model_vvba, extend_inferencedata=True\n",
        "    )"
      ],
      "metadata": {
        "id": "iT8sSH2y4Mcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# posterior predictive check\n",
        "az.plot_ppc(idata_vvba, num_pp_samples=200, colors=[\"C1\", \"C0\", \"C1\"])"
      ],
      "metadata": {
        "id": "eMH-GxYi4PoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the best estimate of the mode thus far. Its still got some funky behavior on the left tail--the next step would be to try a T distribution as a likelihood."
      ],
      "metadata": {
        "id": "oJY-OPwlRZdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comments on Priors in Linear Models\n",
        "\n",
        "I've neglected any commentary on prior selection until now, in order to focus on the structure and ramifications of the linear model. But now I'll comment on lightly informative priors, which I will continue to use this entire unit, and which are a good default.\n",
        "\n",
        "Lightly informative priors tell our model where a parameter IS NOT as opposed to strongly informative priors, which tell a model where a parameter IS. For example, all priors thus far have had a mean of 0, despite none of our actual slopes or y-ints  estimates ended being all that close to 0. We do this so that our model doesn't have to jump around to outrageously large numbers to check our parameter. What's an outragouesly large number? We let our model know based on the standard deviation of the prior.\n",
        "\n",
        "The point: our priors don't have to be right, they just have to give our model a general idea of there to look, and where not to look.\n",
        "\n",
        "That said, if you do have information about where to expect the y-intercept or slope in a linear model, then use that information to select a prior. Again, that's called an informative prior, and it would likely have the same nice properties of the lightly/weakly informative priors I've used thus far."
      ],
      "metadata": {
        "id": "3tdL3RzxRo8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchical Linear Models (1 predictor)\n",
        "\n",
        "Now that I've talked about priors and linear models, let's investigate a context where it might make sense to use hyper priors and hierarchies.\n",
        "\n",
        "That context is the individual NBA player statistics from the 2023/2024 season. I show the whole dataset in the next few code blocks, but we'll only concern ourselves with 3 things: FT%, FG%, and Pos (player position).\n"
      ],
      "metadata": {
        "id": "4YBb559OSAPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some context on FT%, FG%, and Pos:\n",
        "\n",
        "FT% is the same as in the Wembanyama example. We'll use it as the predictor, since free throws are nominally unaffected by any other aspects of a game, or even a season.\n",
        "\n",
        "With that reasoning FT% could be a way to predict FG%, which is the accuracy of a players shooting during game play. FG% is basically how likely a player is to score points when they try to score points--and since scoring points is the method of winning, it's a statistic of great interest to teams and players. FG% is affected by all aspects of a game, so having something less variable to predict it, like FT%, would be nice.\n",
        "\n",
        "The final wrinkle is Pos, or player position. As we saw in the soccer example, players with different roles on a team have different relationships to scoring."
      ],
      "metadata": {
        "id": "3x3vR5o0oaOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data into a pandas dataframe, and print it\n",
        "url_basketball = 'https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/basketball2324.csv'\n",
        "#I could have put the url string directly into pd.read_csv()\n",
        "basketball = pd.read_csv(url_basketball,dtype={'Pos':'category'})"
      ],
      "metadata": {
        "id": "sXWTI13RcC3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Click here for more info on query](https://stackoverflow.com/questions/67341369/pandas-why-query-instead-of-bracket-operator)."
      ],
      "metadata": {
        "id": "QJmqL8JSev4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#only look at players who played more than 400 minutes\n",
        "basketball = basketball.query('MP > 400')\n",
        "#remove players who never missed a free throw\n",
        "basketball = basketball.query('`FT%` != 1.0')"
      ],
      "metadata": {
        "id": "2jPNHW7Gau9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basketball"
      ],
      "metadata": {
        "id": "3VCNwBwjcU2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make scatter plot of FT% and FG% for NBA players form the 23/24 season\n",
        "plt.scatter(basketball['FT%'] , basketball['FG%'],)\n",
        "\n",
        "#label axes\n",
        "plt.xlabel(\"FT%\")\n",
        "plt.ylabel(\"FG%\")"
      ],
      "metadata": {
        "id": "0fX6TrhNLcfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task8**:\n",
        "\n",
        "Look at the scatter plot above. Describe any patterns you see."
      ],
      "metadata": {
        "id": "fSWaxMsZuKwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Annser8**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "T31ByS-4uaOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is an array of the basketball positions,\n",
        "#except they are given an int (number) label instead of a string (of letters)\n",
        "pos_idx = basketball.Pos.cat.codes.values\n",
        "\n",
        "#an array of the strings that represent position\n",
        "pos_codes = basketball.Pos.cat.categories\n",
        "\n",
        "#puts coords in a dict, so pymc can read them\n",
        "bb_coords = {\"pos\": pos_codes}"
      ],
      "metadata": {
        "id": "t8xyjUCqudP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set up the 12 plots\n",
        "_, ax = plt.subplots(2, 6, figsize=(10, 5), sharex=True, sharey=True)\n",
        "ax = np.ravel(ax)\n",
        "\n",
        "#make a scatter plot for each position\n",
        "for i, p in enumerate(pos_codes):\n",
        "    ax[i].scatter(\n",
        "        x = basketball.query('Pos == @p')['FT%'],\n",
        "        y =  basketball.query('Pos == @p')['FG%'],\n",
        "        marker=\".\")\n",
        "    ax[i].set_title(f\"{p}\")"
      ],
      "metadata": {
        "id": "k_VDPE_ddLYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task9**:\n",
        "\n",
        "Describe the relationship between FT% (on the x axis), and FG% (y axis) for each of the positions.\n",
        "\n",
        "Focus your effects on the 5 pure positions, C (center), PF (power forward), SF (small forward), SG (shooting guard), and PG (point guard).\n",
        "\n",
        "Also: why am I less concerned with you describing the remaining combo postions?"
      ],
      "metadata": {
        "id": "ubv5kf5OvDkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer9**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "zhYffNBdv1Of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've seen the raw data nd visualized it, let's formalize our guesses by building a model.\n",
        "\n",
        "First, I'll run seperate linear regressions on each of the positions.\n",
        "\n",
        "I let all the priors and likelihoods be normal. Again, a normality assumption is often a great place to start when building a model.\n",
        "\n",
        "Additionally, I used a straight line function $α + βx$, as there doesn't appear to be a curve trend to my eye in any of the raw scatter plots."
      ],
      "metadata": {
        "id": "XpeClfl2i4MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nh for non hierarchical\n",
        "with pm.Model(coords=bb_coords) as nh_model:\n",
        "    #priors\n",
        "    α = pm.Normal(\"α\", mu=0, sigma=3, dims=\"pos\")\n",
        "    β = pm.Normal(\"β\", mu=0, sigma=10, dims=\"pos\")\n",
        "    σ = pm.HalfNormal(\"σ\", 1)\n",
        "\n",
        "    #likelihood\n",
        "    #the linear part of the model is put directly into mu\n",
        "    y = pm.Normal(\"y_pred\",\n",
        "                  mu=α[pos_idx] + β[pos_idx] * basketball['FT%'],\n",
        "                  sigma=σ,\n",
        "                  observed=basketball['FG%'])\n",
        "    idata_nh_b = pm.sample()"
      ],
      "metadata": {
        "id": "f6PLC_SHhYrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plots the y-int and slope for each position\n",
        "az.plot_forest(idata_nh_b, var_names=[\"α\", \"β\"], combined=True, figsize=(10, 9))"
      ],
      "metadata": {
        "id": "1Hzq97PukI34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above are the slopes and y-intercepts our model came out with, for each of the positions."
      ],
      "metadata": {
        "id": "V3RtqX2UxdRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is code to plot those lines (a y-int and a slope is a line) against the raw data. For now, foucs on the output,the graphs; you can return to parse the code later."
      ],
      "metadata": {
        "id": "47usRq5rxmZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set out our 12 plots\n",
        "_, ax = plt.subplots(2, 6, figsize=(12, 7), sharex=True, sharey=True)\n",
        "ax = np.ravel(ax)\n",
        "\n",
        "#set up x values. Used to graph line of best fit\n",
        "x_range = np.linspace(basketball['FT%'].min(), basketball['FT%'].max(), 10)\n",
        "\n",
        "#grab posterior info\n",
        "posterior = az.extract(idata_nh_b)\n",
        "\n",
        "#for each position\n",
        "for i, p in enumerate(pos_codes):\n",
        "    #plot the raw data\n",
        "    ax[i].scatter(\n",
        "        x = basketball.query('Pos == @p')['FT%'],\n",
        "        y = basketball.query('Pos == @p')['FG%'],\n",
        "        marker=\".\")\n",
        "\n",
        "    #gives our graphs titles and labels\n",
        "    ax[i].set_title(f\"{p}\")\n",
        "    ax[i].set_xlabel(\"FT%\")\n",
        "    ax[i].set_ylabel(\"FG%\", labelpad=10, rotation=0)\n",
        "\n",
        "    #grab the slope and y-int\n",
        "    alphas = posterior[\"α\"].sel(pos=p)\n",
        "    betas = posterior[\"β\"].sel(pos=p)\n",
        "    alpha_m = alphas.mean(\"sample\").item()\n",
        "    beta_m = betas.mean(\"sample\").item()\n",
        "\n",
        "    #plot the mean line, or line of best fit\n",
        "    ax[i].plot(x_range, alpha_m + beta_m * x_range, c=\"k\")\n",
        "\n",
        "    #plot a 94% HDI of the line of best fit.\n",
        "    az.plot_hdi(x_range, alphas + betas * xr.DataArray(x_range).transpose(), ax=ax[i])\n",
        "\n",
        "    #set the limits of our graphs' window\n",
        "    plt.xlim(basketball['FT%'].min() - 0.01, basketball['FT%'].max() + 0.01)\n",
        "    plt.ylim(basketball['FG%'].min() - 0.01, basketball['FG%'].max() + 0.01)"
      ],
      "metadata": {
        "id": "oQflXyp9kyBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task10**:\n",
        "\n",
        "Describe the posteriors for each of the graphs above, and interpret them in context.\n",
        "\n",
        "As a reminder, the context is: is FT% helpful in predicting FG%, within a given basketball position?"
      ],
      "metadata": {
        "id": "GiiDMl02XQR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer10**:\n",
        "\n",
        "[*write your answer here*]\n",
        "\n"
      ],
      "metadata": {
        "id": "P2gPBQFgYEFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even with priors, our estimates for the combo groups are widely uncertain.\n",
        "\n",
        "Our solution to that problem is to use even more priors! specifically, we'll use hyper priors in a heirarchical model framework.\n",
        "\n",
        "Look below to see what that looks like in the linear regression context."
      ],
      "metadata": {
        "id": "rUbRZE4zaeUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#h for hierarchical\n",
        "with pm.Model(coords=bb_coords) as h_model:\n",
        "    # hyper-priors\n",
        "    α_μ = pm.Normal(\"α_μ\", mu=basketball['FG%'].mean(), sigma=3)\n",
        "    α_σ = pm.HalfNormal(\"α_σ\", 5)\n",
        "    β_μ = pm.Normal(\"β_μ\", mu=0, sigma=3)\n",
        "    β_σ = pm.HalfNormal(\"β_σ\", sigma=5)\n",
        "\n",
        "    # priors\n",
        "    α = pm.Normal(\"α\", mu=α_μ, sigma=α_σ, dims=\"pos\")\n",
        "    β = pm.Normal(\"β\", mu=β_μ, sigma=β_σ, dims=\"pos\")\n",
        "    σ = pm.HalfNormal(\"σ\", 5)\n",
        "\n",
        "    #likelihood\n",
        "    #the linear part of the model is put directly into mu\n",
        "    y = pm.Normal(\"y_pred\",\n",
        "                  mu=α[pos_idx] + β[pos_idx] * basketball['FT%'],\n",
        "                  sigma=σ,\n",
        "                  observed=basketball['FG%'])\n",
        "\n",
        "    idata_h_b = pm.sample(target_accept=0.99)"
      ],
      "metadata": {
        "id": "nl14aTHdTXSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task11**:\n",
        "\n",
        "Describe similarities and differences between the code for h_model and the code for nh_model."
      ],
      "metadata": {
        "id": "kCs23tJ9bojI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer11**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "aupHwijqb5jR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm not going to make any graphs for h_model, because\n",
        "\n",
        "- this model has a tendency to generate divergneces in the its markov chains, even with increasing the target acceptance rate.\n",
        "- we have a solution\n",
        "\n",
        "h_model frames our guess as for the slope of one position as informed by knowledge of the other positions' slopes\n",
        "\n",
        "hoff_model (hierarchical offest) frames our guess as a deflection from the overall mean.\n",
        "\n",
        "Those two definitions are mathematicall equivalent. So why use one over the other?\n",
        "\n",
        "The offest model is easier for our markov chains to munch on. For example, the model below tends to produce less divergences than the previous one."
      ],
      "metadata": {
        "id": "Y9U721LTc933"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hoff for hierarchical offset\n",
        "with pm.Model(coords=bb_coords) as hoff_model:\n",
        "    # hyper-priors\n",
        "    α_μ = pm.Normal(\"α_μ\", mu=basketball['FG%'].mean(), sigma=3)\n",
        "    α_σ = pm.HalfNormal(\"α_σ\", 5)\n",
        "    β_μ = pm.Normal(\"β_μ\", mu=0, sigma=3)\n",
        "    β_σ = pm.HalfNormal(\"β_σ\", sigma=5)\n",
        "\n",
        "    # priors\n",
        "    α = pm.Normal(\"α\", mu=α_μ, sigma=α_σ, dims=\"pos\")\n",
        "    σ = pm.HalfNormal(\"σ\", 5)\n",
        "\n",
        "    #prior of beta is different; now, we predict how far our guess is from the\n",
        "    #overall average\n",
        "    β_offset = pm.Normal(\"β_offset\", mu=0, sigma=3, dims=\"pos\")\n",
        "    β = pm.Deterministic(\"β\", β_μ + β_offset * β_σ, dims=\"pos\")\n",
        "\n",
        "    #likelihood\n",
        "    #the linear part of the model is put directly into mu\n",
        "    y = pm.Normal(\"y_pred\",\n",
        "                  mu=α[pos_idx] + β[pos_idx] * basketball['FT%'],\n",
        "                  sigma=σ,\n",
        "                  observed=basketball['FG%'])\n",
        "\n",
        "    idata_hoff_b = pm.sample(target_accept = 0.99)"
      ],
      "metadata": {
        "id": "AyOpwZjxZT0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#displays the y-ints and slopes of the offset hierachical model\n",
        "az.plot_forest(idata_hoff_b, var_names=[\"α\", \"β\"], combined=True, figsize=(10, 9))"
      ],
      "metadata": {
        "id": "zYFkFLBAcwSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set up plots\n",
        "_, ax = plt.subplots(2, 6, figsize=(12, 7), sharex=True, sharey=True)\n",
        "ax = np.ravel(ax)\n",
        "#x values for putting through our linear equation, in order to graph the line\n",
        "x_range = np.linspace(basketball['FT%'].min(), basketball['FT%'].max(), 10)\n",
        "#grab the posterior\n",
        "posterior = az.extract(idata_hoff_b)\n",
        "\n",
        "#plot all 12 lines against the data\n",
        "for i, p in enumerate(pos_codes):\n",
        "    #plot raw data\n",
        "    ax[i].scatter(\n",
        "        x = basketball.query('Pos == @p')['FT%'],\n",
        "        y = basketball.query('Pos == @p')['FG%'],\n",
        "        marker=\".\")\n",
        "    #titles and labels\n",
        "    ax[i].set_title(f\"{p}\")\n",
        "    ax[i].set_xlabel(\"FT%\")\n",
        "    ax[i].set_ylabel(\"FG%\", labelpad=10, rotation=0)\n",
        "    #grab posterior info\n",
        "    alphas = posterior[\"α\"].sel(pos=p)\n",
        "    betas = posterior[\"β\"].sel(pos=p)\n",
        "    alpha_m = alphas.mean(\"sample\").item()\n",
        "    beta_m = betas.mean(\"sample\").item()\n",
        "    #plot the posterior\n",
        "    ax[i].plot(x_range, alpha_m + beta_m * x_range, c=\"k\")\n",
        "    az.plot_hdi(x_range, alphas + betas * xr.DataArray(x_range).transpose(), ax=ax[i])\n",
        "    #set window size\n",
        "    plt.xlim(basketball['FT%'].min() - 0.01, basketball['FT%'].max() + 0.01)\n",
        "    plt.ylim(basketball['FG%'].min() - 0.01, basketball['FG%'].max() + 0.01)"
      ],
      "metadata": {
        "id": "OTG2vbRoc0nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task12**:\n",
        "\n",
        "How does the hoff_model posterior differ from the nh_model posterior?"
      ],
      "metadata": {
        "id": "4ZUnoJwwhKDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer12**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "ChGseMeOiVni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task13**:\n",
        "\n",
        "Which model is better: nh_model, or hoff_model? Why?\n",
        "\n",
        "Be sure to reference the graphs in your justification."
      ],
      "metadata": {
        "id": "54xYEk2TkDaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer13**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "ZJhkZoWpkP9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification with Logistic Regression\n",
        "\n",
        "Regression--what we've been doing this whole unit--predicts a continuous value based on input variable(s).\n",
        "\n",
        "Classification assigns an exact label to a given data point. A common way to create a classification algorithm is to take a regression model, and apply a decision boudary to it. That decision boundary then lets us assign a label.\n",
        "\n",
        "Example time:"
      ],
      "metadata": {
        "id": "Qdkf40PCm9p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data into a pandas dataframe, and print it\n",
        "url_iris = 'https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/iris.csv'\n",
        "#I could have put the url string directly into pd.read_csv()\n",
        "iris = pd.read_csv(url_iris,dtype={'species':'category'})"
      ],
      "metadata": {
        "id": "Mldza-O5l3hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris"
      ],
      "metadata": {
        "id": "IAmyHBFHWNf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is the classic iris data set, which has info about three species of iris flowers. We'll try to predict whether a flower is setosa or versicolor using only sepal_length."
      ],
      "metadata": {
        "id": "Cf9di4lcm71d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removes virginica\n",
        "iris_sv = iris.query(\"species == ('setosa', 'versicolor')\")\n",
        "\n",
        "#numpy array of the two species, but\n",
        "#but with 0 for each setosa and 1 for each veriscolor\n",
        "species_codes = pd.Categorical(iris_sv[\"species\"]).codes\n",
        "\n",
        "#makes a numpy array of the sepal length values\n",
        "sepal_len = iris_sv.sepal_length.values\n",
        "\n",
        "#here I normalize the sepal length values.\n",
        "#This makes it easier for pymc to process\n",
        "sepal_len_c = sepal_len - sepal_len.mean()"
      ],
      "metadata": {
        "id": "Ud37m6i9mQkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plots setosa in blue and versicolor in orange.\n",
        "#Darker dots means more points are there\n",
        "plt.scatter(sepal_len_c, species_codes, marker=\".\", color=[f\"C{x}\" for x in species_codes], alpha = 0.2)"
      ],
      "metadata": {
        "id": "PnnrSSajisc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that 0 on the graph above represents the average sepal length of setosa and veriscolor iris flowers in the data set."
      ],
      "metadata": {
        "id": "aeARVSd8noFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task14**:\n",
        "\n",
        "Pick a sepal length from graph above as your boundary or decision value for when an iris is setosa or versicolor.\n",
        "\n",
        "Bascially, eyeball a solution to our problem."
      ],
      "metadata": {
        "id": "JPS7LDsIn7rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anwser14**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "RXZvF37loQ3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've looked at the raw data and thought about the problem, let's cook up a model to make our decisions for us.\n",
        "\n",
        "New things to look for:\n",
        "- we transform our linear equation using something called a sigmoid (AKA logistic) curve. You'll see what it looks like soon.\n",
        "- we add in a boundary decision value, $\\frac{-α}{β}$. This tells our model that 50% probability of being one species or the other is our cut off.\n",
        "- we use a bernoulli likelihood, since a flower from our spliced data set is either setosa (0) or versicolor (1)."
      ],
      "metadata": {
        "id": "4gdrGVo8ohql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#li stands for logistic iris\n",
        "with pm.Model() as model_li:\n",
        "\n",
        "    #priors\n",
        "    α = pm.Normal(\"α\", mu=0, sigma=1)\n",
        "    β = pm.Normal(\"β\", mu=0, sigma=5)\n",
        "\n",
        "    #linear part of the model\n",
        "    μ = α + sepal_len_c * β\n",
        "\n",
        "    #the linear part, mu, is transformed via a sigmoid curve\n",
        "    θ = pm.Deterministic(\"θ\", pm.math.sigmoid(μ))\n",
        "\n",
        "    #this lets our model find a boudary value\n",
        "    #which allows us to do classificiation\n",
        "    #It is not neccesary for the model to run\n",
        "    bd = pm.Deterministic(\"bd\", -α / β)\n",
        "\n",
        "    #likelihood\n",
        "    y = pm.Bernoulli(\"y\", p=θ, observed=species_codes)\n",
        "\n",
        "    idata_li = pm.sample()"
      ],
      "metadata": {
        "id": "MwfrLxYwok7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grab the posterior\n",
        "posterior = idata_li.posterior\n",
        "theta = posterior[\"θ\"].mean((\"chain\", \"draw\"))\n",
        "\n",
        "#this will help us tanslate from the standardized data back to the raw data\n",
        "idx = np.argsort(sepal_len_c)\n",
        "\n",
        "#set up the plot\n",
        "_, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "#plot the posterior\n",
        "ax.plot(sepal_len_c[idx], theta[idx], color=\"C0\", lw=2)\n",
        "ax.vlines(posterior[\"bd\"].mean((\"chain\", \"draw\")), 0, 1, color=\"C2\", zorder=0)\n",
        "\n",
        "#bd for boundary\n",
        "#these two lines plot our boundary or decision line, which is what we could use\n",
        "#to predict the sepcies of a flower using sepal length\n",
        "bd_hdi = az.hdi(posterior[\"bd\"])\n",
        "ax.fill_betweenx([0, 1], bd_hdi[\"bd\"][0], bd_hdi[\"bd\"][1], color=\"C2\", alpha=0.6, lw=0)\n",
        "\n",
        "#scatter plot of raw data\n",
        "ax.scatter(sepal_len_c, species_codes, marker=\".\", color=[f\"C{x}\" for x in species_codes], alpha = 0.2)\n",
        "\n",
        "#plots the sigmoid regression\n",
        "az.plot_hdi(sepal_len_c, posterior[\"θ\"], color=\"C0\", ax=ax, fill_kwargs={\"lw\": 0})\n",
        "\n",
        "#labels\n",
        "ax.set_xlabel('sepal length')\n",
        "ax.set_ylabel(\"θ\", rotation=0)\n",
        "\n",
        "# # use original scale for xticks\n",
        "locs, _ = plt.xticks()\n",
        "ax.set_xticks(locs, np.round(locs + sepal_len_c.mean(), 1))"
      ],
      "metadata": {
        "id": "d04Eldu8f4Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task15**:\n",
        "\n",
        "Describe the above graph, and how it answers our question:\n",
        "\n",
        "What value of sepal length should we use to decide if an iris is a setosa or a versicolor?"
      ],
      "metadata": {
        "id": "n4UqCpM9qUPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer15**:\n",
        "\n",
        "[*write your answer here*]"
      ],
      "metadata": {
        "id": "FFeYNswJquF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "We did a lot this unit. We:\n",
        "\n",
        "- learned what a linear model is, and how to make one\n",
        "- played with different likelihood and priors for various linear models\n",
        "- generalized the linear part of our model to most curves, like square root, and a new one, the sigmoid.\n",
        "- learned how to model variable variance (AKA heteroskedasticity) with babies\n",
        "- walked through an example of a linear hierarchical model with NBA data, and discussed why we would and would not want to use one.\n",
        "- learned a way to turn a regression model into a classification, flower identifying machine\n",
        "\n",
        "Mainly, we learned ways to use some source of data (how hot it is outside) to predict a different source of data (how many people are riding bikes). That task, predicting something given something else, fits such a broad range of problems that the rest of this course is adding complexity to what you've seen in this unit.\n",
        "\n",
        "For example: next unit, we'll try to predict how many people are renting bikes using not only the temperature, but *gasp* the humidity and time of day.\n",
        "\n"
      ],
      "metadata": {
        "id": "9GNh18sBrSu3"
      }
    }
  ]
}