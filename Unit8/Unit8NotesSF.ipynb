{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedarredondo/data-science-fundamentals/blob/main/Unit8/Unit8NotesSF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymc-bart\n",
        "!pip install preliz"
      ],
      "metadata": {
        "id": "xh8BCvWb4Kei",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YSkoMLi-xN5"
      },
      "outputs": [],
      "source": [
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import pymc_bart as pmb\n",
        "import preliz as pz\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 8: Bayesian Additive Regression Trees (BART)\n",
        "\n",
        "Bayesian Additive Regression Trees (BART) can be thought of as a fast approximation of Gaussian Processes (GPs). The specific way BART models work has various limitations, quirks, and benefits; we'll dicuss them all in this unit.\n",
        "\n",
        "We'll learn:\n",
        "- what a decision tree is\n",
        "- how BART models work, and their relationshipp to decision trees\n",
        "- how to implement BART in PyMC\n",
        "- Partial Dependence Plots (pdp)\n",
        "- Individual Conditional Expecation plots (ice)\n",
        "- variable importance (vi) plots\n",
        "\n",
        "Let's get started."
      ],
      "metadata": {
        "id": "j1DVpKSi_COG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theoretical Background: Trees and Forests\n",
        "\n"
      ],
      "metadata": {
        "id": "HA8uPRyqB3ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Trees\n",
        "\n",
        "Think of decision trees as flow charts (the technical term is a graph), but with several restrictions.\n",
        "- Each node can have only one 'parent' node, but many children.\n",
        "-We'll focus on binary decision trees, where each node as one parent, and two or zero children\n",
        "- There is a special node called the 'root' node, with no parents\n",
        "- Each layer of the tree sorts the predictor values into subsets of the predicted values\n",
        "\n",
        "All thos points are best illustrated with an example.\n",
        "\n",
        "For our example, we'll use some data on octupus beaks, and try to use upper beak length (predictor) to predict total beak length (predictor).\n"
      ],
      "metadata": {
        "id": "PNf6ytgaFRQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree"
      ],
      "metadata": {
        "id": "_RmFmcxrUjaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make sure to read my comments!**"
      ],
      "metadata": {
        "id": "IEQzQkeOdCJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data on octupus beaks\n",
        "octps = pd.read_csv('https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/refs/heads/main/Data/octopusbeakweight_nlin.csv')"
      ],
      "metadata": {
        "id": "nVXoH7LhVN7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X is our predictor variable, upper beak measurment\n",
        "X = octps[\"upBeak\"].to_numpy().reshape(-1, 1)\n",
        "#Y is the predicted variable, total beak weight\n",
        "Y = octps[\"totWt\"].to_numpy()"
      ],
      "metadata": {
        "id": "qMUMSHUdVWHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fits a decision tree. This is analogous to running pm.sample,\n",
        "#except it only finds the mean. Kinda like that, anyway.\n",
        "dtree = tree.DecisionTreeRegressor(max_depth=2)\n",
        "octtree = dtree.fit(X,Y)\n",
        "\n",
        "#uses the fitted model to predict total weight for various\n",
        "#unseen upper beark measurements\n",
        "#This is analogous to a posterior predictive distribution,\n",
        "#if we only found the posterior predictive mean...\n",
        "#...kinda.\n",
        "X_test = np.arange(75, 250, 1)[:, np.newaxis]\n",
        "y = octtree.predict(X_test)"
      ],
      "metadata": {
        "id": "OfU5LwG1Uk1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the graph (flow chart-y) version of the decision tree."
      ],
      "metadata": {
        "id": "0G98-S7KdnAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree.plot_tree(octtree)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wRmlZlS5WI66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below graph is similar to if I'd plot just the posterior predictive mean."
      ],
      "metadata": {
        "id": "fmAm7NloduPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.scatter(X, Y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"actual measurements\")\n",
        "plt.plot(X_test, y, color=\"cornflowerblue\", linewidth=2)\n",
        "plt.xlabel(\"upBeak\")\n",
        "plt.ylabel(\"totWt\")\n",
        "plt.title(\"Decision Tree Regression\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PL1gk26uWsnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task1**:\n",
        "\n",
        "The graph (flow chart-y) version of the tree has four nodes with no children, often called 'leaf' nodes.\n",
        "\n",
        "Each of those nodes something that says \"value =\", then a number.\n",
        "\n",
        "What are those numbers, and what do they have to do with the psuedo-posterior predictive mean from the scatter plot?"
      ],
      "metadata": {
        "id": "X2hx8Bi8dNYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task2**:\n",
        "\n",
        "There are three other nodes in the graph (flow chart-y) version of the decision tree.\n",
        "\n",
        "These start with a text that reads \"x[0] <=\" followed by a number.\n",
        "\n",
        "What do these numbers mean, and what do they have to do with the psuedo-posterior predictive mean from the scatter plot?"
      ],
      "metadata": {
        "id": "tzFG7Owtf5Jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task3**:\n",
        "\n",
        "There are two others numbers in each node: \"squared error\" and samples.\n",
        "\n",
        "What do those mean?"
      ],
      "metadata": {
        "id": "UB-Y5Vxwh1R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task4**:\n",
        "\n",
        "Change max_depth to equal 5 in the above code, and the recreate both graphs.\n",
        "\n",
        "What changes?\n",
        "\n",
        "What did max_depth do?"
      ],
      "metadata": {
        "id": "t0Ch6RBqi3ET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task5**:\n",
        "\n",
        "Was increasing the depth of the tree a good idea for this data set?\n",
        "\n",
        "Why or why not? Try to use one of \"underfitting\" or \"overfitting\" in your answer."
      ],
      "metadata": {
        "id": "5OWtVX6Kjjym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task6:**\n",
        "\n",
        "Name some pros and cons to using a decision tree. Think about what they do better than other models you've seen, and what they do worse."
      ],
      "metadata": {
        "id": "5df3eyd9sj6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests (RF) and BART\n",
        "\n",
        "A randomized decision tree on it's own needs lots of care and direction to make sure it doesn't overfit--but a whole bunch of decision trees with randomized root nodes, added together, police themselves. Any method that combines the results of multiple randomized decision trees is called a Random Forest (RF). RFs turn out to be pretty accurate, and also one of the more interpretable model types, thanks to being built from decision trees. Increasing the number of trees can make RFs very accurate, and RFs can be exceptionally fast to fit if the number of trees isn't too big.\n",
        "\n",
        "Why is combining trees so much better? If we ensure that there's a diversity of trees (ensure they aren't all making similar decisions), then each tree is picking up on a different pattern in the data. Combining the results of the trees combines all the patterns each tree picked up, resulting in a model that \"knows\" about all of the patterns and can make better predictions. This is called ensemble learning, where a bunch of weak learners are combined to result in more accurate predictions.\n",
        "\n",
        "At least, that's what we hope happens. There's no guranttee that each tree will pick up something unique about the data. Luckily, we can ecourage our trees to find different patterns by using a BART model.\n",
        "\n",
        "Here's how: In addition to the randomized root nodes of a a basic RF, BART builds its trees in a sort of sequential manner. BART actually starts with all the trees it needs, but then randomly alters each tree based on the other trees. The \"based on the other trees\" bit ensures that BART is encouraging the growth of different trees. This idea is called boosting.\n",
        "\n",
        "BART goes further though; it selects each change to one of its trees as part of an MCMC. This ensures that the whole BART model is a sample from some posterior distribution of possible random forests. You can find a slightly more in depth summary of the BART model as described [here](https://www.youtube.com/watch?v=xWhPwHZF4c0).\n",
        "\n",
        "There's a final step also unique to BART, that the video above doesn't really cover: BART puts regularizing priors on the depth of each decision tree, and on the magnitude of the leaf nodes. The priors over the depth helps ensure that all the trees will be shallow, or that the depth will be much less than the number of data points.\n",
        "Priors on the leaf nodes ensures that our model only explores near the actual data; the leaf node priors are more traditional regularizing priors.\n",
        "\n"
      ],
      "metadata": {
        "id": "lIQ8tVi0EeIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why all the background on how BART works?\n",
        "\n",
        "BART is a particularly good example of the algorithm buidling process. It started with something that works with many flaws (Decision Trees), imporved to something that works pretty well on its on (RFs), but kept adding things that helped either reduce underfitting (boosting) or reduce overfitting (MCMC, priors). The result is a flexible model that can accurately fit data *and* that we don't have to spend forever tuning.\n",
        "\n",
        "That push to make a better algorithm happened to create something that approximates a known mathematical object: a BART model with infinite trees is a nowhere differentiable Guassian Process. BART models are often much faster than any type of GP, and *way* easier to use. They are also simple to interpret, b/c their building blocks, decision trees, are easy to interpret.\n",
        "\n",
        "One last time: people invented a good model by considering how to best balance underfitting, overfitting, speed, and interpretability."
      ],
      "metadata": {
        "id": "vpFlXZGGPFCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing BART in PyMC\n",
        "\n",
        "Armed with knowledge of how BART works and why we should use it, let's build a model with it."
      ],
      "metadata": {
        "id": "gEMTqno1R233"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Octupus BART"
      ],
      "metadata": {
        "id": "FOFGhg9NYxLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First is some preliz stuff I used to come up with a decent likelihood. You don't need to run it, I just thought I'd leave it here so y'all could see a bit of my thought process."
      ],
      "metadata": {
        "id": "qa2y9WPoyKtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import preliz as pz"
      ],
      "metadata": {
        "id": "tms_aVlhdL83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pz.maxent(pz.Gamma(), 100, 3000, 0.85)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CN0IMulUdOGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pz.Gamma(1,5).plot_pdf()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YWOoioPXhvuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We never looked at the actual dataframe, which was bad form. Let's do it now."
      ],
      "metadata": {
        "id": "wvvtSR6yyitx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(octps.totWt)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Jbv45fftaqW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "octps.head()"
      ],
      "metadata": {
        "id": "Mf1dFXbdZDXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code is just here to remind you what X and Y are. It also has features commented out; that can be a nice way to put multiple predictiors into a BART model."
      ],
      "metadata": {
        "id": "1kDv0A-szCpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#features = [\"upBeak\", \"loBeak\", \"latWall\"]\n",
        "\n",
        "X = octps[\"upBeak\"].to_numpy().reshape(-1, 1)\n",
        "Y = octps[\"totWt\"].to_numpy()"
      ],
      "metadata": {
        "id": "4tBAeUWJY-oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task7**:\n",
        "\n",
        "Examine the model below. Is the BART part of the model a prior, or a likelihood?\n",
        "\n",
        "Were GPs priors, or likelihoods?"
      ],
      "metadata": {
        "id": "z-nAExz9zWmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with pm.Model() as model_octps:\n",
        "    s = pm.HalfNormal('s',1000)\n",
        "    μ_ = pmb.BART(\"μ_\", X, np.log(Y), m=50)\n",
        "    μ = pm.Deterministic(\"μ\",pm.math.exp(μ_))\n",
        "    y = pm.Gamma(\"y\", mu=μ, sigma=s,  observed=Y)\n",
        "    idata_octps = pm.sample(compute_convergence_checks=False)"
      ],
      "metadata": {
        "id": "ObO9bNQRZSmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate the posterior predictive dist\n",
        "pm.sample_posterior_predictive(idata_octps,model_octps, extend_inferencedata=True)\n",
        "az.plot_ppc(idata_octps, num_pp_samples=100, colors=[\"C1\", \"C0\", \"C1\"])"
      ],
      "metadata": {
        "id": "tVpHj5WTnJqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_mean = idata_octps.posterior[\"μ\"]\n",
        "\n",
        "μ_hdi = az.hdi(ary=idata_octps, group=\"posterior\", var_names=[\"μ\"], hdi_prob=0.74)\n",
        "\n",
        "pps = az.extract(\n",
        "    idata_octps, group=\"posterior_predictive\", var_names=[\"y\"]\n",
        ").T"
      ],
      "metadata": {
        "id": "t3zi1I9ay1TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argsort(X[:, 0])\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X[:, 0],\n",
        "    y=pps,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.93,\n",
        "    fill_kwargs={\"alpha\": 0.3, \"label\": r\"Posterior Predictive $93\\%$ HDI\"},\n",
        ")\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X[:, 0],\n",
        "    y=posterior_mean,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.74,\n",
        "    fill_kwargs={\"alpha\": 0.6, \"label\": r\"Mean $74\\%$ HDI\"},\n",
        ")\n",
        "ax.plot(octps[\"upBeak\"], octps[\"totWt\"], \"o\", c=\"C0\", label=\"Raw Data\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "ax.set(\n",
        "    title=\"Posterior Predictive\",\n",
        "    xlabel=\"upBeak\",\n",
        "    ylabel=\"totWt\",\n",
        ");"
      ],
      "metadata": {
        "id": "5Avcfnqx3MzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: Because BART models--and random forests in general--are just a bunch of horzontal and vertical lines taped together, they have a bias towards \"flatness\" (or parallelness to the predictor) on the left and right edges of any predictive plot."
      ],
      "metadata": {
        "id": "09iQPYGv_oKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task8**:\n",
        "\n",
        "Comment on the fit of the above posterior predictive check. Is it good, bad, or ugly?"
      ],
      "metadata": {
        "id": "XS3awOgu3w1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counties Data, PDPs, ICE, and VI plots\n",
        "\n",
        "Trigger warning: This analysis covers suicide, and it attempts to find things that influence suicide rates by county in the USA. If you need support in dealing with this topic, reach out. Your voice matters!\n",
        "\n",
        "You can find a full explanation of the counties dataset [here](https://github.com/evangambit/JsonOfCounties?tab=readme-ov-file). The person who curated and intially cleaned the data (Evan Gambit?), did great work, and made this analysis possible. I do not know them, but I am grateful.\n",
        "\n",
        "I've clean the data to make it more amenable to analyze suicide and poverty, and then I use a BART to model predict suicide rate using teen birth rate, poverty rate, self reported number of mentally unhealthy days, proportion with less than a high school level education, and the proportion that voted for the GOP in the 2020 election.\n",
        "\n",
        "We will make use of some new tools, called partial dependence plots (pdp) and individual conditional expectation (ice) plots, to determine which variables are associated with a higher suicide rate by county.\n",
        "\n",
        "Pdps will allow us to see how each predictor variable relates to suicide rate--assuming there are no interactions between predictor variables.\n",
        "\n",
        "Ice plots will allows to see if their are interactions, and even give us a hint as to the nature of the interactions.\n",
        "\n",
        "Vi plots will give us an idea of which variables contribute more predictive power.\n",
        "\n",
        "Altogether, the plots will give us a clearer picture of what our model is, what it values, and what relationships exist in the data."
      ],
      "metadata": {
        "id": "F2Iu-Sh7OrcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data and Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "2IGgHty-QQ7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First though, is a bit of data cleaning. Not all the columns seem relevant to predict suicide; I arbitrarily chose the variables below.\n",
        "\n",
        "Someone could easily decide a different set are relevant, and end up with a different analysis than me."
      ],
      "metadata": {
        "id": "JCovQ7y1gi1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counties = pd.read_csv('https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/refs/heads/main/Data/counties.csv')"
      ],
      "metadata": {
        "id": "JgctW_UZ_zDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clmn_list = ['name',\n",
        "             'state',\n",
        "             'population/2019',\n",
        "             'health/Violent Crime Rate',\n",
        "             'life-expectancy',\n",
        "             'health/Teen Birth Rate',\n",
        "             'poverty-rate',\n",
        "             'health/Average Number of Mentally Unhealthy Days',\n",
        "             'health/% Excessive Drinking',\n",
        "             'edu/bachelors+',\n",
        "             'avg_income',\n",
        "             'deaths/suicides',\n",
        "             'edu/less-than-high-school',\n",
        "             'elections/2020/gop',\n",
        "             ]\n",
        "sad_stats =  counties.loc[:,clmn_list]"
      ],
      "metadata": {
        "id": "vphc4l6WBxcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also drop any rows that don't have data for any one of the variables. This removed around 300 to 400 hundred counties. Perhaps having data for those counties would alter the ensuing results?\n",
        "\n",
        "I won't attempt to answer that question for two reasons:\n",
        "- creating a model for the missing data would require me to learn about every county that we dropped, and/or learning about the data collection method, which I don't have time for.\n",
        "- We have a large enough set of data that we can still learn something about the world."
      ],
      "metadata": {
        "id": "MP03Exj2hMTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdsts_nona = sad_stats.dropna()"
      ],
      "metadata": {
        "id": "MVF4Y8NxHz0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I next weight the raw counts by county population. This is not representative of proportion of population (its unclear how deaths by suicide influnce the offical poulation count, for example), but it does allow me to compare large counties to small counties. Unlike with raw counts."
      ],
      "metadata": {
        "id": "9RMTZab4jh4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdsts_nona.insert(sdsts_nona.shape[1], \"suicide/pop\", sdsts_nona['deaths/suicides']/sdsts_nona['population/2019'])\n",
        "sdsts_nona.insert(sdsts_nona.shape[1], \"<hs/pop\", sdsts_nona['edu/less-than-high-school']/sdsts_nona['population/2019'])\n",
        "sdsts_nona.insert(sdsts_nona.shape[1], \"gop/pop\", sdsts_nona['elections/2020/gop']/sdsts_nona['population/2019'])\n",
        "sdsts_nona.insert(sdsts_nona.shape[1], \"bachelors+/pop\", sdsts_nona['edu/bachelors+']/sdsts_nona['population/2019'])\n",
        "sdsts_nona.drop(['deaths/suicides', 'edu/less-than-high-school','elections/2020/gop','edu/bachelors+'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "bX7cJa8gcGAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this column name is too dang long, so I shorten it\n",
        "sdsts_nona.rename(columns={'health/Average Number of Mentally Unhealthy Days':'mentUnhealth','health/Teen Birth Rate':'teenBirth'}, inplace=True)"
      ],
      "metadata": {
        "id": "y2trb1W0SWO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, I did some exploratory data analyisi to see which variables might actually be associated with suicide rate. I didn't keep all my plots from that, but here's a sample of some I thought were interesting."
      ],
      "metadata": {
        "id": "e8LmZHFOk1xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdsts_nona.head()"
      ],
      "metadata": {
        "id": "lXor0uS8Te_t",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data = sdsts_nona, y = 'suicide/pop', x = 'poverty-rate', hue = 'gop/pop')"
      ],
      "metadata": {
        "id": "2hjzOOmG2RBB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdsts_nona.plot(y = 'suicide/pop', x = 'gop/pop', kind = \"scatter\")"
      ],
      "metadata": {
        "id": "EBdioQ3Gfsxk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prep Data for BART Model"
      ],
      "metadata": {
        "id": "EIbkkQ1TQZx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the exploratory data analysis, I only used teen birth rate, poverty rate, number of mentally unhealthy days, proportion with less than a highschool education, and proportion who voted for the GOP in the 2020 election."
      ],
      "metadata": {
        "id": "K_SQ5QFWlJSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I combine all my predictor variables into one dataframe\n",
        "X = sdsts_nona[[\n",
        "             'teenBirth',\n",
        "             'poverty-rate',\n",
        "             'mentUnhealth',\n",
        "             '<hs/pop',\n",
        "             'gop/pop',\n",
        "             ]]\n",
        "Y = sdsts_nona['suicide/pop'].to_numpy()"
      ],
      "metadata": {
        "id": "pP-QbzbJOjqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART Model"
      ],
      "metadata": {
        "id": "JD0e8C4UT6mL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the model, in a svelte six lines of code.\n",
        "\n",
        "Suicide rate is again all positive (like octupus beak weight), so I use a gamma likelihood. There are also the same log and exp shenanigans from before; these serve to make suicide rate go into the negatives, b/c BART is a real numbers 4eva type of gal.\n",
        "\n",
        "But seriously, the pymc implementation of BART needs to have its predicted range over positive and negative values, so we have to do this for any all positive support likelihood."
      ],
      "metadata": {
        "id": "pQ3V1dnhlg0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with pm.Model() as model_counties:\n",
        "    s = pm.HalfNormal('s',20) #sigma = 20 is a blind guess\n",
        "    μ_ = pmb.BART(\"μ_\", X, np.log(Y), m=50) #log around Y\n",
        "    μ = pm.Deterministic(\"μ\",pm.math.exp(μ_)) #exp it all once BART is done\n",
        "    y = pm.Gamma(\"y\", mu=μ, sigma=s,  observed=Y) #likelihood\n",
        "    idata_counties = pm.sample(compute_convergence_checks=False)"
      ],
      "metadata": {
        "id": "dEj2p2LUT56p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm.sample_posterior_predictive(idata_counties,model_counties, extend_inferencedata=True)"
      ],
      "metadata": {
        "id": "D7mqir1AUIaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of y'all may look at this and think \"inverse gamma would definitely be better\" but every time I tried that likelihood, if gave me strange behavior. Maybe you'll have better luck if you try."
      ],
      "metadata": {
        "id": "0eUah6BlnAZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = az.plot_ppc(idata_counties, num_pp_samples=100, colors=[\"C1\", \"C0\", \"C1\"])\n",
        "ax.set_xlim(0.0,0.0005)"
      ],
      "metadata": {
        "id": "eebD_7u6dMrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next two code blocks let you plot the posterior predictive for a single one of the variables versus suicide rate. This is less useful, in my opinion, than what follows."
      ],
      "metadata": {
        "id": "DmVNFokfniq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_mean = idata_counties.posterior[\"μ\"]\n",
        "\n",
        "μ_hdi = az.hdi(ary=idata_counties, group=\"posterior\", var_names=[\"μ\"], hdi_prob=0.74)\n",
        "\n",
        "pps = az.extract(\n",
        "    idata_counties, group=\"posterior_predictive\", var_names=[\"y\"]\n",
        ").T"
      ],
      "metadata": {
        "id": "a_jnIgSvafae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argsort(X.to_numpy()[:, 4]) #grab fifth row\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X.to_numpy()[:, 4],#grab fifth row\n",
        "    y=pps,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.93,\n",
        "    fill_kwargs={\"alpha\": 0.3, \"label\": r\"Posterior predictive $93\\%$ HDI\"},\n",
        ")\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X.to_numpy()[:, 4],#grab fifth row\n",
        "    y=posterior_mean,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.74,\n",
        "    fill_kwargs={\"alpha\": 0.6, \"label\": r\"Mean $74\\%$ HDI\"},\n",
        ")\n",
        "ax.plot(sdsts_nona['gop/pop'], sdsts_nona[\"suicide/pop\"], \"o\", c=\"C0\", label=\"Raw Data\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "ax.set(\n",
        "    title=\"Posterior Predictive\",\n",
        "    xlabel=\"gop/pop\",\n",
        "    ylabel=\"suicide/pop\",\n",
        ");"
      ],
      "metadata": {
        "id": "MLF8z4oScaIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partial Dependence Plots (pdp)\n",
        "\n",
        "This is the beginning of the exciting stuff.\n",
        "\n",
        "The five graphs below show the association between a predictor varible and the predicted variable, marginalized over the other variables.\n",
        "\n",
        "Why is that exciting?\n",
        "\n",
        "It means that these plots show the affect of the proportion who voted for the GOP in 2020 conditioning on the effect of poverty, teen birth rate, reported mental health, and under-education...\n",
        "\n",
        "...**ASSUMING** that all those predictor variables are mostly independent."
      ],
      "metadata": {
        "id": "-8H0eJSXn6ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_pdp(μ_, X, Y, grid=(1, 5), func=np.exp, figsize = (12,6))"
      ],
      "metadata": {
        "id": "WjobGRSEWwxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task10**:\n",
        "\n",
        "Interpret the gop/pop versus suicide rate partial dependence plot, assuming approximate independence between the predictor variables.\n",
        "\n",
        " What is the trend?\n",
        "\n",
        " Why?"
      ],
      "metadata": {
        "id": "tBqfGJtqpibo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Individual Conditional Expectation (ice) Plots\n",
        "\n",
        "Let's drop the assumption that each of the predictor variables is predictor variables is indepentent--b/c they aren't.\n",
        "\n",
        "For example, education level and GOP voting tendency have had a pretty stable negative association in last couple of decades. And poverty, teen birth rate, education, and mental health are all likely interacting with each other as well.\n",
        "\n",
        "The first step to account for that is an ice plot.\n",
        "\n",
        "Look at the lines that make up the ice plots below. The average of all those lines makes up the pdp from above; each line n the ice plot represents a single observation. All those lines averaged are teh pdp.\n",
        "\n",
        "Individual predictions per data point help us see trends that aren't present in the average.\n",
        "\n",
        "What do we look for? Parallel-ness: if all the lines in each plot are parallel, then when can ignore the ice plot, and go back to the pdp"
      ],
      "metadata": {
        "id": "wpwL_AYCvYfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_ice( μ_, X, Y, grid=(1, 5), func=np.exp, figsize = (12,6))"
      ],
      "metadata": {
        "id": "d6wlj9RzXo5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 11**:\n",
        "\n",
        "Are the lines within each plot parallel, in the ice plots above?\n",
        "\n",
        "If they aren't parallel, then describe and interpret the areas in which the lines within the plots are not parallel.\n",
        "\n",
        "Hint: The lines aren't parallel. Get to describing and interpreting."
      ],
      "metadata": {
        "id": "EdcdpLE9xSed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable Importance (vi) Plots\n",
        "\n",
        "VI plots, in theory, are supposed to tell us which variables to select if all we care about is prediction accruacy and speed.\n",
        "\n",
        "For example, the plot below seems to indicate we could probably drop teen birth rate and number of bad mental health days, and get a similar level of performance. It also seems to indicate that gop tendancy and poverty rate give the biggest boosts in performance, as there jumps up in $R^{2}$ are largest.\n",
        "\n",
        "There are lots of caveats to those interpretations though.\n",
        "\n",
        "- The order in which the model calculated the $R^{2}$ values matters--and the order is not staable (i.e. it's different from run to run)\n",
        "- we're using $R^{2}$ instead of elpd_loo, b/c pymc hasn't implemented elpd_loo for BART.\n",
        "- this plot does not show us variable interactions. BART is almost certainly applying the equivalent of variable interactions, but our only hint about what those interactions are comes from the ice plots above"
      ],
      "metadata": {
        "id": "V3UBDZH-1aNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vi_counties = pmb.compute_variable_importance(idata_counties, μ_, X)"
      ],
      "metadata": {
        "id": "gC68I6Z0CvJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_variable_importance(vi_counties)"
      ],
      "metadata": {
        "id": "K7krnmh9XNvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion to predicting sucicide rate by county using BART\n",
        "\n"
      ],
      "metadata": {
        "id": "IER7owki42Bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task12**:\n",
        "\n",
        "Summarize everything we learned from and about model_counties."
      ],
      "metadata": {
        "id": "VpXStAtA7Tkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "Once again, we've covered a lot of ground.\n",
        "\n",
        "We put in the work to start understanding the concept of decision trees, how they can flexibly adapt to data, in an interpretable way.\n",
        "\n",
        "Then we jumped into a theoretical disscussion on how to supe up our decions trees, making them more powerful.\n",
        "\n",
        "Those fancy, aggregated decision trees were called BART, and they allowed us to quickly create a model which could determine an appropriate non-linear shape, all on its own.\n",
        "\n",
        "Next, we pivoted to a different application of BART, with multiple predictiors. It still helped to have BART be able to capture non-linear trends, but we also used pdps, ice plots, and vi plots to find out the relationships between several predictor variables and suicide rate by county.\n",
        "\n",
        "We were able to determine some interesting associations, and also learn that there were many interactions between our predictor variables.\n",
        "\n",
        "BART is a powerful, flexible, and interpretable model. We can use the framework to quickly create a good prediction engine for simple to complex data. We can also use it to learn about relationships in our data."
      ],
      "metadata": {
        "id": "xX6IZpTRBWQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What's next?\n",
        "\n",
        "So, is there anything BART (or random forests) can't do?\n",
        "\n",
        "BART and RFs can do almost anything a GP can do, which is a lot. But there are a couple model types that are doing something different than GPs. Some of those are the unspervised learning methods--mostly PCA and UMAP. The other is a big buzz word: deep neural networks (DNN). The next two units will tackle rudimentary NNs, and then DNNs, and we'll probably get lost along the way. But who knows, maybe it'll be fun too."
      ],
      "metadata": {
        "id": "6DMhEQABDDN-"
      }
    }
  ]
}